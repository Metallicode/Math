{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZBlTZbG+wlk9LCKYf8TZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/MAE_Vs_MSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mean Absolute Error (MAE) and Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "Vv48Ru49LqLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Error (MAE) and Mean Squared Error (MSE) are both metrics that measure the average magnitude of errors between predicted and observed values, but they do so in different ways. Let's dive into each:\n",
        "\n",
        "### Mean Absolute Error (MAE):\n",
        "\n",
        "```\n",
        "MAE = 1/N * sum(abs(y_i - y_hat_i))\n",
        "```\n",
        "\n",
        "**Pros**:\n",
        "1. **Interpretability**: The result is in the same unit as the original data, making it easy to understand. If your data is in dollars, a MAE of 5 means you're off by 5 dollars on average.\n",
        "2. **Equal Weight**: It treats all errors equally. An error of 10 has twice the weight of an error of 5.\n",
        "\n",
        "**Cons**:\n",
        "1. **Less Sensitive to Outliers**: If having large errors is undesirable in your application, MAE might not penalize them heavily enough.\n",
        "\n",
        "### Mean Squared Error (MSE):\n",
        "\n",
        "```\n",
        "MSE = 1/N * sum(y_i - y_hat_i)**2\n",
        "```\n",
        "\n",
        "**Pros**:\n",
        "1. **Penalizes Larger Errors**: Errors are squared, so larger errors have a disproportionately large impact. This can be useful if it's important to avoid large errors.\n",
        "2. **Differentiability**: The squared term makes the function smooth, which is beneficial for optimization algorithms that rely on derivatives, like gradient descent.\n",
        "\n",
        "**Cons**:\n",
        "1. **Less Interpretability**: The result is in squared units of the original data, which can be harder to interpret directly.\n",
        "2. **Sensitive to Outliers**: A single very large error can dominate the MSE, which might cause the model to overfit to outliers.\n",
        "\n",
        "### Which to Use?\n",
        "\n",
        "- **Nature of the Problem**: For some problems, larger errors might be particularly undesirable, making MSE a better choice.\n",
        "  \n",
        "- **Interpretability**: If you need an error metric that's easy for non-experts to understand, MAE might be preferable.\n",
        "\n",
        "- **Outliers in Data**: If your data has a lot of outliers, MSE might be overly influenced by them. In such cases, MAE or even other robust metrics like the Huber loss could be better choices.\n",
        "\n",
        "### Knowledge Gained:\n",
        "\n",
        "- **MAE**: Gives a direct average error magnitude. It tells you, on average, how much your predictions are off from the actual values.\n",
        "  \n",
        "- **MSE**: Gives more weight to larger errors. It provides insight into the quality of predictions in scenarios where bigger mistakes are more critical.\n",
        "\n",
        "In practice, it can be beneficial to consider both metrics, along with domain knowledge and the specific objectives of the analysis, when evaluating and comparing models."
      ],
      "metadata": {
        "id": "riwCY2IJKydP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj22hX3LKsg4",
        "outputId": "c4334ddd-2474-45a5-921f-d436682c1be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.5\n",
            "MSE: 0.375\n"
          ]
        }
      ],
      "source": [
        "def mean_absolute_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Mean Absolute Error (MAE) between true and predicted values.\n",
        "    \"\"\"\n",
        "    mae = sum(abs(y_t - y_p) for y_t, y_p in zip(y_true, y_pred)) / len(y_true)\n",
        "    return mae\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Mean Squared Error (MSE) between true and predicted values.\n",
        "    \"\"\"\n",
        "    mse = sum((y_t - y_p)**2 for y_t, y_p in zip(y_true, y_pred)) / len(y_true)\n",
        "    return mse\n",
        "\n",
        "# Example usage:\n",
        "y_true = [3, -0.5, 2, 7]\n",
        "y_pred = [2.5, 0.0, 2, 8]\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "print(\"MSE:\", mean_squared_error(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5k03xUCMBy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}