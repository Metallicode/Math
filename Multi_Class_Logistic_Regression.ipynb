{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mYUGfMBMgzm0",
        "uhF9PO6mmz1Y",
        "hJH-HbEcvI_U",
        "NHNnfxWwu_PQ",
        "TVE1npSbs0n-",
        "3t9Sm2IEuuBO"
      ],
      "authorship_tag": "ABX9TyNKlMwnHZNXUxP0yKtXbH+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Multi_Class_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MultiClass Logistic Regression"
      ],
      "metadata": {
        "id": "mYUGfMBMgzm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Iris flower DataSet"
      ],
      "metadata": {
        "id": "uhF9PO6mmz1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AW-o0neMgqyN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get the data"
      ],
      "metadata": {
        "id": "hJH-HbEcvI_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"iris.csv\")"
      ],
      "metadata": {
        "id": "l3yy9bpSnTOl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop([\"species\"], axis=1)\n",
        "y = df[\"species\"]"
      ],
      "metadata": {
        "id": "8Ne7aMdlnaK1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split and Scale"
      ],
      "metadata": {
        "id": "4KZq0b0BvLnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import  StandardScaler"
      ],
      "metadata": {
        "id": "3JrPx7Gmn1pC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "kinvquMKoTTl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "8eM22p5Cpl5G",
        "outputId": "56517d94-a46c-46e5-8849-358f7311f397"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "8jCObjkpptp1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model and GridSearch"
      ],
      "metadata": {
        "id": "SkLNUXCNvRed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "t8xBwLyJrKhC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_model = LogisticRegression(solver=\"saga\", multi_class=\"ovr\", max_iter=5000, verbose=0)"
      ],
      "metadata": {
        "id": "Bf8cSvZ_tAt4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GridSearch parameters"
      ],
      "metadata": {
        "id": "NHNnfxWwu_PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "penalty = [\"l1\",\"l2\",\"elasticnet\"]\n",
        "l1_ratio = np.linspace(0,1,20)\n",
        "C = np.logspace(0,10,20)\n",
        "\n",
        "param_grid = {\n",
        "    \"penalty\" : penalty,\n",
        "    \"l1_ratio\" : l1_ratio,\n",
        "    \"C\" : C\n",
        "}"
      ],
      "metadata": {
        "id": "QMk6MAdWvG4g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_model = GridSearchCV(logistic_model, param_grid=param_grid)"
      ],
      "metadata": {
        "id": "Irm_poPewFsf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train"
      ],
      "metadata": {
        "id": "uYS5a-d3wk8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tables.tests.common import verbosePrint\n",
        "grid_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "0wuWxhT5wkl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "tNWDIwi-xOQk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_model.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l95nbLCWxOJE",
        "outputId": "3ae0295a-6844-41e7-c928-0377085ac020"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 3.3598182862837818, 'l1_ratio': 0.0, 'penalty': 'l1'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = grid_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "5Dmfd1UHypUc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_preds, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRKdWSvkyvrT",
        "outputId": "900026ec-883c-4f25-f415-2bd12471c0ee"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_preds, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UZV9jryy5OS",
        "outputId": "8e91d16c-9cf0-4b72-81a8-7f5c5fdf7b4f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19,  0,  0],\n",
              "       [ 0, 13,  0],\n",
              "       [ 0,  0, 13]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Solver"
      ],
      "metadata": {
        "id": "TVE1npSbs0n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"solver\" in the context of logistic regression refers to the optimization algorithm used to find the parameters (e.g., weights and bias) that minimize the loss function. Logistic regression involves finding the maximum likelihood estimate of the parameters, and this optimization task is done using various algorithms.\n",
        "\n",
        "Different solvers might be preferable depending on the nature of the data and the problem. Here are the solvers provided in `scikit-learn`'s implementation of logistic regression:\n",
        "\n",
        "1. **liblinear**:\n",
        "    - Algorithm: It's a library for large linear classification which uses a coordinate descent (CD) algorithm.\n",
        "    - Pros: Good for small datasets.\n",
        "    - Limitations: Does not handle multinomial loss natively; instead, it uses a one-vs-rest scheme.\n",
        "\n",
        "2. **lbfgs**:\n",
        "    - Algorithm: Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno. It approximates the second derivative matrix updates with an estimation from the last few gradient evaluations.\n",
        "    - Pros: Works well for small to medium datasets.\n",
        "    - Limitations: Might be slow for very large datasets.\n",
        "\n",
        "3. **newton-cg**:\n",
        "    - Algorithm: Newton's method which also uses second-order derivative information.\n",
        "    - Pros: Can converge faster than other methods because it leverages the second order derivative.\n",
        "    - Limitations: Needs to compute the second derivative matrix, which is computationally expensive for large datasets.\n",
        "\n",
        "4. **sag**:\n",
        "    - Algorithm: Stochastic Average Gradient descent. It uses a stochastic gradient descent approach, and only a random subset of the samples is used to compute the gradients at each step.\n",
        "    - Pros: Fast for large datasets.\n",
        "    - Limitations: Requires a lot of iterations to converge.\n",
        "\n",
        "5. **saga**:\n",
        "    - Algorithm: Extension of `sag` which also allows for L1 regularization.\n",
        "    - Pros: Useful for large datasets with L1 regularization.\n",
        "    - Limitations: Like `sag`, it requires a lot of iterations to converge.\n",
        "\n",
        "When to use which solver:\n",
        "\n",
        "- For **small datasets**, `liblinear` is often good enough.\n",
        "- For **multinomial loss** or **multi-class problems**, prefer `lbfgs`, `sag`, `saga`, or `newton-cg`.\n",
        "- For **large datasets**, `sag` or `saga` might be the best options because of their efficiency with large sample sizes.\n",
        "\n",
        "It's also worth noting that, depending on the regularization and data, not all solvers can handle all types of regularization, and sometimes specific solvers are better suited for specific regularizations."
      ],
      "metadata": {
        "id": "c1vGpTnQs4db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\"solver\" and \"penalty\""
      ],
      "metadata": {
        "id": "3t9Sm2IEuuBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Solver**:\n",
        "   - **What it is**: The optimization algorithm used to find the parameters (coefficients) of the logistic regression model that minimize the loss function.\n",
        "   - **Purpose**: Solves the optimization problem, i.e., finds the best weights/coefficients for the logistic regression model.\n",
        "   - **Common Options**: `liblinear`, `lbfgs`, `newton-cg`, `sag`, `saga` (as detailed in the previous answer).\n",
        "\n",
        "2. **Penalty**:\n",
        "   - **What it is**: The type of regularization applied to the logistic regression model.\n",
        "   - **Purpose**: Regularization is a technique used to prevent overfitting by adding a penalty to the magnitude of coefficients. Depending on the type of penalty, it can drive some coefficients to zero (L1) or just shrink coefficients (L2).\n",
        "   - **Common Options**:\n",
        "     - `l1`: Lasso regularization. Can drive some coefficients to zero, leading to feature selection. Typically used with solvers like `liblinear` and `saga`.\n",
        "     - `l2`: Ridge regularization. Shrinks the coefficients but doesn't set any to zero. Compatible with a wider range of solvers, like `lbfgs`, `newton-cg`, `sag`, `saga`, and `liblinear`.\n",
        "     - `elasticnet`: A combination of L1 and L2 regularization. It seeks to blend the properties of both Lasso and Ridge. Typically used with the `saga` solver.\n",
        "     - `none`: No regularization.\n",
        "\n",
        "The interplay:\n",
        "\n",
        "- Not all combinations of solvers and penalties are compatible in `scikit-learn`. For example, the `liblinear` solver doesn't support the `elasticnet` penalty.\n",
        "- Regularization (specified by the \"penalty\") can help in preventing overfitting, especially in scenarios where the number of features is high relative to the number of training samples. The strength of the regularization is typically controlled by another parameter, often called `C` or `alpha` in `scikit-learn`.\n",
        "- The choice of solver can affect the speed of convergence and accuracy of the solution. Some solvers might work better for certain types of data or penalty types.\n",
        "\n",
        "In summary, while the solver determines how the model will optimize its weights, the penalty determines how those weights will be regularized to prevent overfitting."
      ],
      "metadata": {
        "id": "52WxHNScuorg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ucpi3vYDs3U8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}