{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ+lk2t+OskXNPRG0w8D7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Feature_Scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Scaling"
      ],
      "metadata": {
        "id": "QKFLhP3-BQw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) is an essential preprocessing step in many machine learning algorithms, especially for optimization-based methods such as gradient descent. Let's dive into the concept:\n",
        "\n",
        "**1. What is it?**\n",
        "Feature scaling is the process of normalizing or standardizing the range of independent variables or features in the data. The primary goal is to make sure features have a similar scale so that no particular feature dominates the others in influencing the model due to its scale.\n",
        "\n",
        "**2. Common Methods:**\n",
        "- **Min-Max Scaling (Normalization)**: This scales features so they are in the range [0, 1]. The formula is:\n",
        "  \n",
        "  new_x = (x-min(x)) / (max(x) - min(x))\n",
        "  \n",
        "- **Standardization (Z-score normalization)**: This method scales features so they have a mean (μ) of 0 and a standard deviation (σ) of 1. The formula is:\n",
        "  \n",
        "  new_x = (x - μ) / σ\n",
        "\n",
        "### Connection to Gradient Descent:\n",
        "\n",
        "**1. Faster Convergence:**\n",
        "When we use gradient descent to update weights, features with a smaller range would have a smaller gradient step, while features with a larger range would take a larger step. If one or more features have vastly larger scales than others, then gradient descent can oscillate and take a long time to converge. When all features are on a similar scale, the gradient tends to point more directly towards the minimum, allowing gradient descent to converge more quickly.\n",
        "\n",
        "**2. More Robust Convergence:**\n",
        "Unscaled or poorly scaled data can lead to a situation where the contour plot of the cost function is elongated. Gradient descent can overshoot and oscillate around the minimum rather than converging. Feature scaling ensures a more \"circular\" contour, leading to a smoother path to the minimum.\n",
        "\n",
        "**3. Avoiding Dominant Features:**\n",
        "If one feature has a range of 0-1 and another feature has a range of 0-1000, the latter feature can disproportionately affect the prediction, even if it's not necessarily more important. Scaling ensures each feature has an equal initial influence on the model.\n",
        "\n",
        "### In Summary:\n",
        "Feature scaling standardizes the range of features, ensuring that no feature artificially dominates others due to its scale. In the context of gradient descent, this results in a more direct path to the cost function's minimum and faster convergence. For deep learning models and neural networks where there could be thousands of gradient updates, feature scaling becomes especially crucial to ensure timely and stable convergence."
      ],
      "metadata": {
        "id": "kNoI3NxOCCD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generally we do not need to scale the labels, just the features.**"
      ],
      "metadata": {
        "id": "KBwKj4mjJbS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "1W_hXou3JBvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalization"
      ],
      "metadata": {
        "id": "GkgAWQpNItHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdsrVJmHBK8w"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Sample data\n",
        "data = np.array([50, 60, 70, 80, 90])\n",
        "\n",
        "# Compute the minimum and maximum\n",
        "data_min = np.min(data)\n",
        "data_max = np.max(data)\n",
        "\n",
        "# Normalize the data\n",
        "normalized_data = (data - data_min) / (data_max - data_min)\n",
        "\n",
        "print(\"Original Data: \", data)\n",
        "print(\"Normalized Data: \", normalized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##using sklearn\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = np.array([50, 60, 70, 80, 90]).reshape(-1, 1)  # Reshaping because MinMaxScaler expects 2D array\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(data)\n",
        "#print(data)\n",
        "print(\"Original Data: \", data.flatten())\n",
        "print(\"Normalized Data: \", normalized_data.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwgqPHuKI1xJ",
        "outputId": "4f42d888-6c7c-4758-b7d6-ad26756afdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[50]\n",
            " [60]\n",
            " [70]\n",
            " [80]\n",
            " [90]]\n",
            "Original Data:  [50 60 70 80 90]\n",
            "Normalized Data:  [0.   0.25 0.5  0.75 1.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Standardization"
      ],
      "metadata": {
        "id": "04WD_J6UG7nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data = np.array([50, 60, 70, 80, 90])\n",
        "\n",
        "# Compute the mean and standard deviation\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "# Standardize the data\n",
        "standardized_data = (data - mean) / std_dev\n",
        "\n",
        "print(\"Original Data: \", data)\n",
        "print(\"Standardized Data: \", standardized_data)\n",
        "print(\"Mean of Standardized Data: \", np.mean(standardized_data))\n",
        "print(\"Standard Deviation of Standardized Data: \", np.std(standardized_data))\n"
      ],
      "metadata": {
        "id": "zvfgZDzeG6z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Sklearn"
      ],
      "metadata": {
        "id": "p2Bwk2MNmNOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scalar = StandardScaler()\n",
        "\n",
        "data = np.array([50, 60, 70, 80, 90])\n",
        "\n",
        "data = data.reshape(-1, 1)\n",
        "\n",
        "scalar.fit(data)\n",
        "\n",
        "scalar.transform(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwqQyNH-mMmx",
        "outputId": "6c7d6585-fd6d-467a-ab30-dbe042784a51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.41421356],\n",
              "       [-0.70710678],\n",
              "       [ 0.        ],\n",
              "       [ 0.70710678],\n",
              "       [ 1.41421356]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rY__pyHVm_k-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}