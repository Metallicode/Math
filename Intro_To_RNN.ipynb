{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaz4nK8Djufyr6SsklNHZj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Intro_To_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intro To RNN"
      ],
      "metadata": {
        "id": "eK4rTMeH6SrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNNs Basics Terms:**\n",
        "\n",
        "\n",
        "**Recurrent Neural Networks (RNNs)**: A class of neural networks designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or spoken words. It's called \"recurrent\" because they perform the same operation for every element of a sequence, with the output depending on previous computations (a loop).\n",
        "\n",
        "**Sequence to Sequence**\n",
        "- **Sequence-to-Sequence (Seq2Seq)**: An architecture used mainly for translation tasks or any task that maps an input sequence to an output sequence. It comprises two main parts: an encoder (reads the input and compresses the information into a context vector) and a decoder (reads the context vector to produce the output).\n",
        "\n",
        "**Sequence to Vector**\n",
        "- This usually refers to the encoder part of the Seq2Seq model, where an input sequence is transformed into a fixed-size context vector.\n",
        "\n",
        "**Vanishing Gradients**\n",
        "- As RNNs are trained, earlier layers may receive very tiny updates to their weights, essentially halting learning. This is caused by the multiplication of many small values (gradients) during backpropagation, which makes the gradients vanish.\n",
        "- The opposite problem, exploding gradients, occurs when gradients become too large, making the network weights update too aggressively.\n",
        "\n",
        "**Various Activation Functions**\n",
        "- **ReLU (Rectified Linear Unit)**: It outputs the input for positive values and zero for negative values. It’s simple and fast, but can sometimes \"die\" during training.\n",
        "- **Leaky ReLU**: A variant of ReLU that has a small positive slope even for negative values, preventing units from \"dying\".\n",
        "- **ELU (Exponential Linear Unit)**: Like ReLU but tends to converge more rapidly and produces better accuracy. For negative inputs, it takes on an exponential curve.\n",
        "\n",
        "**Batch Normalization**\n",
        "- A method to improve training speed and reduce the sensitivity to initialization. It normalizes each input/activation to have a mean of zero and variance of one, using the mean and variance of the current batch's data.\n",
        "\n",
        "**Weights Initialization**\n",
        "- Proper initialization ensures that activations and gradients don’t vanish or explode during initial epochs. Popular techniques include Xavier/Glorot and He initialization.\n",
        "\n",
        "**Gradient Clipping**\n",
        "- A technique used to combat the exploding gradients problem. It involves manually setting a threshold value, and any gradient value that exceeds this threshold is set to the threshold.\n",
        "\n",
        "This summary provides a basic understanding of RNNs and related concepts. However, the field is vast and evolving, and further reading will give you a deeper understanding of each topic."
      ],
      "metadata": {
        "id": "aNGihR-My3FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense"
      ],
      "metadata": {
        "id": "vMox2RKQ1dCj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tDPQGFd5sYL",
        "outputId": "2cd75cce-3a9c-4284-9275-e62a1996ce1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 2s 3ms/step - loss: 0.6289\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0323\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0126\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0058\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 9.2532e-04\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "[[1.5042027]]\n"
          ]
        }
      ],
      "source": [
        "# Generating random data: 10000 sequences of length 5\n",
        "X = np.random.rand(10000, 5, 1)  # 10000 samples, each of length 5 and having 1 feature\n",
        "y = np.sum(X, axis=1) #for each sample (of 5 elements) sum the values\n",
        "\n",
        "# Simple RNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an RNN layer with 10 units\n",
        "model.add(SimpleRNN(10, input_shape=(5, 1)))\n",
        "\n",
        "# Output layer to predict the sum (regression problem)\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "X_new = np.array([[[0.1], [0.2], [0.3], [0.4], [0.5]]])  # Should be close to 1.5\n",
        "print(model.predict(X_new))"
      ],
      "metadata": {
        "id": "Hzty8yoo21Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUhxHytg2uxj",
        "outputId": "f01e9945-03d3-43e9-ea17-7b82c89d14e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###explanation"
      ],
      "metadata": {
        "id": "xIbMxj6t2Lsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.rand(10000, 5, 1)  # 10000 samples, each of length 5 and having 1 feature\n",
        "y = np.sum(X, axis=1)"
      ],
      "metadata": {
        "id": "Bp9Mlmww1bAF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozkYkzqk1kOC",
        "outputId": "94e47f11-33d2-4e43-9e85-8c6d8369c0ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx3EGUQB1lJj",
        "outputId": "ca18edd9-fe38-4cc8-dc1f-853c73151531"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.82023843],\n",
              "       [0.57754691],\n",
              "       [0.0155293 ],\n",
              "       [0.27080635],\n",
              "       [0.92298352]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-23hQul17c_",
        "outputId": "26ab6c51-8021-4018-8ed0-58912a57a09b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.60710452])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufigRotw19a7",
        "outputId": "afcac3bc-0f7e-4058-baa0-a26365780aea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6071045154346812"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Problem With Simple RNN's"
      ],
      "metadata": {
        "id": "15ZWAytF4pbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difficulty in capturing long-term dependencies in vanilla RNNs is often not evident in short sequences like the one used in the example. However, when you deal with longer sequences, the problem becomes more pronounced.\n",
        "\n",
        "To illustrate this, let's modify the simple example:\n",
        "\n",
        "- Instead of summing the entire sequence, we will create a task where the target is influenced by the first and the last elements of the sequence.\n",
        "\n",
        "- We'll increase the sequence length significantly.\n",
        "\n",
        "Here's an illustrative example:\n",
        "\n",
        "**Task**: Given a sequence, the target is the sum of the first and the last element."
      ],
      "metadata": {
        "id": "ggIBbhCj41Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense"
      ],
      "metadata": {
        "id": "gENonhai5NFh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generating random data: 10000 sequences of length 30\n",
        "sequence_length = 30\n",
        "X = np.random.rand(10000, sequence_length, 1)  # 10000 samples\n",
        "y = X[:, 0] + X[:, -1]\n",
        "\n",
        "# Simple RNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an RNN layer with 10 units\n",
        "model.add(SimpleRNN(10, input_shape=(sequence_length, 1)))\n",
        "\n",
        "# Output layer to predict the sum (regression problem)\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=50, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRhELP8m4yaN",
        "outputId": "392e94f1-7c1e-4278-ce9e-c718a5efee5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "313/313 [==============================] - 3s 6ms/step - loss: 0.1748\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0411\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0205\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0133\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0092\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0068\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0053\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0043\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0037\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0031\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.0027\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0024\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0020\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0017\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0016\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0014\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0013\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 9.5416e-04\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 8.8924e-04\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 7.6369e-04\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 6.9802e-04\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 6.4692e-04\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 5.9857e-04\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 5.4359e-04\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 5.2353e-04\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 4.4963e-04\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 4.1771e-04\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 4.0441e-04\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 3.8775e-04\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 3.2894e-04\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 3.9338e-04\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 3.0252e-04\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 3.0970e-04\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 3.1814e-04\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 3.2254e-04\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.9561e-04\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.4852e-04\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.3974e-04\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.5040e-04\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.3798e-04\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.3014e-04\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.5336e-04\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9570e-04\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 1.9530e-04\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 1.8380e-04\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9333e-04\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.0500e-04\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.6920e-04\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.8778e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0fa840a200>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "X_new = np.random.rand(1, sequence_length, 1)\n",
        "true_sum = X_new[0, 0] + X_new[0, -1]\n",
        "predicted_sum = model.predict(X_new)\n",
        "\n",
        "print(f\"True sum: {true_sum[0]}\")\n",
        "print(f\"Predicted sum: {predicted_sum[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5om6lLu5QJn",
        "outputId": "fb06d979-2b52-4275-acfa-c33363d21a43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 166ms/step\n",
            "True sum: 1.1793697607097735\n",
            "Predicted sum: 1.1744807958602905\n"
          ]
        }
      ]
    }
  ]
}