{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aeNgZ3mh91nv"
      ],
      "authorship_tag": "ABX9TyPCYwZa9izjFH4VcHdbJmiO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Boosting_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Methods"
      ],
      "metadata": {
        "id": "VMbmrRWXmma6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Boosting works by combining multiple weak learners to create a strong learner. **A weak learner** is a model that performs only slightly better than random guessing, while **a strong learner** is a model that has high accuracy on the task.*\n",
        "\n",
        "* Sequential Learning: Unlike Random Forests, which train each tree independently, boosting methods train learners sequentially. Each subsequent model is built to correct the mistakes of its predecessor.\n",
        "\n",
        "* Weighted Training Data: After each round, instances that were misclassified by the current weak learner are given higher weights, making it more likely for subsequent learners to focus on them. Similarly, correctly classified instances are given lower weights.\n",
        "\n",
        "* Aggregation: The final prediction is typically a weighted combination of the predictions of all the weak learners. For regression tasks, this is often a weighted sum. For classification tasks, it can be a weighted majority vote.\n",
        "\n",
        "* Regularization via Shrinkage: Boosting algorithms often introduce a learning rate (or shrinkage) parameter. This slows down the learning process by shrinking the contribution of each weak learner, which often leads to better performance and less overfitting.\n",
        "\n",
        "* Strengths: Often provides higher accuracy than other algorithms, especially on tabular data.\n",
        "Effective with imbalanced datasets by focusing more on the underrepresented class.\n",
        "\n",
        "* Challenges: More prone to overfitting, especially with noisy data or when the dataset is small.\n",
        "Typically slower to train than bagging methods like Random Forests due to the sequential nature.\n",
        "Loses some interpretability when compared to a single decision tree.\n",
        "\n",
        "* Hyperparameters: Boosting methods come with their set of hyperparameters (like the depth of the trees, learning rate, and number of trees) that need to be tuned for optimal performance.\n",
        "\n",
        "*In essence, boosting is a versatile and powerful method in the realm of machine learning, particularly for structured/tabular data. It leverages the idea that \"the whole is greater than the sum of its parts,\" turning a series of weak models into a highly accurate combined model.*"
      ],
      "metadata": {
        "id": "DuVPTh7jeBZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Examples of Boosting Algorithms:\n",
        "\n",
        "> **AdaBoost (Adaptive Boosting)**: One of the first successful boosting algorithms. It adjusts the weights of misclassified instances and combines learners through weighted majority voting.\n",
        "\n",
        "> **Gradient Boosting Machines (GBM)**: Builds trees sequentially, where each tree tries to correct the residuals (the differences between the predicted and true values) of the previous one. It generalizes the boosting procedure to optimize arbitrary loss functions.\n",
        "\n",
        "> **XGBoost, LightGBM, CatBoost**: Modern and efficient implementations of gradient boosting that offer faster computation and additional features."
      ],
      "metadata": {
        "id": "srvJhuODe6vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Soft vs. Hard Voting"
      ],
      "metadata": {
        "id": "-oykD9n3hKoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When using ensemble methods, especially in the context of classification, there are two main ways to aggregate predictions from multiple classifiers: soft voting and hard voting.\n",
        "\n",
        "**Hard Voting:**\n",
        "> Each classifier in the ensemble \"votes\" for a class label.\n",
        "The class label that gets the majority of the votes is chosen as the final predicted class.\n",
        "Example: Suppose you have three classifiers and two possible classes (A and B). If the classifiers predict A, A, and B, respectively, then the ensemble prediction using hard voting would be A, as it's the majority class predicted.\n",
        "\n",
        "**Soft Voting:**\n",
        "\n",
        "> Each classifier provides a probability for each class label.\n",
        "The probabilities for each class are averaged across all classifiers.\n",
        "The class with the highest average probability is chosen as the final predicted class.\n",
        "Example: Using the same scenario with two possible classes (A and B), let's say the classifiers provide the following probabilities for class A: 0.7, 0.2, and 0.8. The average probability for class A would be (0.7 + 0.2 + 0.8) / 3 = 0.567. If the average for class B is lower than this, then the ensemble prediction using soft voting would be A.\n",
        "\n",
        "**Which to Use?:**\n",
        "\n",
        "**Soft voting** is generally preferred over hard voting, especially when the classifiers are well-calibrated. The reason is that soft voting takes into account the confidence levels of individual classifiers, leading to potentially more accurate ensemble predictions.\n",
        "\n",
        "**Hard voting** can be more robust in cases where the probability outputs from classifiers aren't reliable or well-calibrated.\n",
        "\n",
        "*In conclusion, while hard voting is a simple majority rule approach, soft voting provides a more nuanced way to aggregate predictions by considering the confidence of individual models."
      ],
      "metadata": {
        "id": "4JMEKojthIf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost"
      ],
      "metadata": {
        "id": "Kn5junWzkISR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "short for \"Adaptive Boosting\".\n",
        "\n",
        "one of the most popular ensemble methods that employs the boosting technique.\n",
        "\n",
        "\n",
        "**AdaBoost in Simple Terms:**\n",
        "\n",
        "> Imagine you're trying to differentiate between cats and dogs, but you're not very good at it. So, you ask a friend (a \"weak learner\") to help. Your friend makes some mistakes but gets some right.\n",
        "\n",
        "> Instead of getting upset at the mistakes, you focus more on the pictures your friend got wrong and ask another friend to try and classify those.\n",
        "\n",
        "> This second friend also makes mistakes, but again, instead of being upset, you focus on the pictures that are still wrong.\n",
        "\n",
        "> You repeat this process with many friends. Each friend tries to correct the mistakes of the previous one.\n",
        "\n",
        "> In the end, you combine all your friends' decisions. Each friend gets a say, but friends who were more confident and accurate have a louder voice in the final decision.\n",
        "\n",
        "*This process is basically AdaBoost! Each friend is a \"weak learner\" (often a simple decision tree). Mistakes made by previous learners are given more emphasis by subsequent learners. In the end, you combine all their decisions for a final, strong decision.*"
      ],
      "metadata": {
        "id": "laayFUe4kMV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost Formula"
      ],
      "metadata": {
        "id": "6PU_YF5rs6R2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Weight Initialization: Each data point is given an equal weight initially.\n",
        "\n",
        "2. For Each Learner:\n",
        "\n",
        "> * Fit the learner on the dataset considering the weights.\n",
        "* Calculate the error of the learner.\n",
        "* Compute the learner's weight in the final decision.\n",
        "Learners with lower error have more weight (a louder voice).\n",
        "* Increase the weights of the misclassified points, so the next learner pays more attention to them.\n",
        "\n",
        "3. Final Output: Combine the decisions of all learners. Each learner's decision is weighted by its accuracy."
      ],
      "metadata": {
        "id": "0gGJ_g6ks7W_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Error of a Learner"
      ],
      "metadata": {
        "id": "Rt3Ed9NRtjr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "learner_error = Sum_of_weights_of_misclassified_points / Total_Sum_of_weights\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "s0_I7BABtm3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Weight of a learner (alpha)"
      ],
      "metadata": {
        "id": "J3s1ypFnuBe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "alpha = (1/2)*natural_log(1-error/error)\n",
        "```"
      ],
      "metadata": {
        "id": "c4YmeIr0uMo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Updating Weights of Data Points"
      ],
      "metadata": {
        "id": "Z8bDot4Ou6m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For correctly classified points:**\n",
        "\n",
        "```\n",
        "new_w = old_w * np.e**-alpha\n",
        "```\n",
        "\n",
        "**For misclassified points:**\n",
        "```\n",
        "new_w = old_w * np.e**alpha\n",
        "```"
      ],
      "metadata": {
        "id": "7Kj6U5F5u-3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final Prediction"
      ],
      "metadata": {
        "id": "nCW2Yinivvsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematically, if h_t(x) is the prediction of the t'th weak learner for an input x and α_t is the weight of that learner, then the aggregated score for x is given by\n",
        "\n",
        "```\n",
        "(Where T is the total number of weak learners)\n",
        "\n",
        "H(x) = sum(α_t*h_t(x) for t in T)\n",
        "\n",
        "Output = Sign(H(x))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "bW2he2AYwXls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple implementation of AdaBoost"
      ],
      "metadata": {
        "id": "IXrwF0EbzCJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Z4GvOXu-0ydg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##np.dot"
      ],
      "metadata": {
        "id": "PggwBWnp9uiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vector dot product x[0]*y[0] + x[1]*y[1]  + x[2]*y[2] .....\n",
        "\n",
        "n_samples = 10\n",
        "sample_weights = np.ones(n_samples) / n_samples\n",
        "#print(sample_weights)\n",
        "dummy_preds = [True, False, True, True, True, True, False, False, True, True]\n",
        "np.dot(sample_weights, dummy_preds)"
      ],
      "metadata": {
        "id": "y2krinQX0pnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost"
      ],
      "metadata": {
        "id": "aeNgZ3mh91nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Toy dataset: 10 data points and 2 features\n",
        "X = np.array([[1, 2], [2, 3], [3, 1], [4, 2], [5, 2], [6, 1], [7, 2], [8, 3], [9, 1], [10, 2]])\n",
        "y = np.array([-1, -1, -1, -1, 1, 1, -1, 1, 1, 1])\n",
        "\n",
        "# Parameters\n",
        "n_samples, n_features = X.shape\n",
        "n_learners = 10\n",
        "learners = []\n",
        "learner_weights = []\n",
        "sample_weights = np.ones(n_samples) / n_samples\n",
        "\n",
        "for i in range(n_learners):\n",
        "    # Fit a weak learner\n",
        "    learner = DecisionTreeClassifier(max_depth=1)\n",
        "    learner.fit(X, y, sample_weight=sample_weights)\n",
        "\n",
        "    # Predict using the weak learner\n",
        "    predictions = learner.predict(X)\n",
        "\n",
        "    # Compute the error\n",
        "    misclassified = (predictions != y)\n",
        "    ##the dot product efficiently computes the numerator of the error formula.\n",
        "    error = np.dot(sample_weights, misclassified) / np.sum(sample_weights)\n",
        "\n",
        "    print(f\"Iteration {i + 1} - Error: {error:.3f} - Predictions: {predictions}\")\n",
        "\n",
        "    if error == 0:\n",
        "        print(\"Perfect classifier found!\")\n",
        "        break\n",
        "    elif error >= 0.5:\n",
        "        print(\"Weak learner is worse than random!\")\n",
        "        continue\n",
        "\n",
        "    # Compute the weak learner's weight\n",
        "    alpha = 0.5 * np.log((1 - error) / error)\n",
        "\n",
        "    # Update data point weights\n",
        "    sample_weights *= np.exp(-alpha * y * predictions)\n",
        "    #normalization step\n",
        "    sample_weights /= np.sum(sample_weights)\n",
        "\n",
        "    # Save the learner and its weight\n",
        "    learners.append(learner)\n",
        "    learner_weights.append(alpha)\n",
        "\n",
        "\n",
        "# Final prediction\n",
        "output = np.zeros(n_samples)\n",
        "for alpha, learner in zip(learner_weights, learners):\n",
        "    output += alpha * learner.predict(X)\n",
        "final_predictions = np.sign(output)\n",
        "\n",
        "print(\"True labels      :\", y)\n",
        "print(\"AdaBoost Predictions:\", final_predictions)\n",
        "\n",
        "final_predictions * y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxfsLt1Jvkmp",
        "outputId": "9effe35c-4c24-48ac-8642-f5c480ca4558"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 - Error: 0.100 - Predictions: [-1 -1 -1 -1  1  1  1  1  1  1]\n",
            "Iteration 2 - Error: 0.111 - Predictions: [-1 -1 -1 -1 -1 -1 -1  1  1  1]\n",
            "Iteration 3 - Error: 0.219 - Predictions: [ 1  1  1  1  1  1 -1 -1 -1 -1]\n",
            "Iteration 4 - Error: 0.180 - Predictions: [-1 -1 -1 -1  1  1  1  1  1  1]\n",
            "Iteration 5 - Error: 0.195 - Predictions: [-1 -1 -1 -1 -1 -1 -1  1  1  1]\n",
            "Iteration 6 - Error: 0.189 - Predictions: [ 1  1  1  1  1  1 -1 -1 -1 -1]\n",
            "Iteration 7 - Error: 0.192 - Predictions: [-1 -1 -1 -1  1  1  1  1  1  1]\n",
            "Iteration 8 - Error: 0.191 - Predictions: [-1 -1 -1 -1 -1 -1 -1  1  1  1]\n",
            "Iteration 9 - Error: 0.191 - Predictions: [ 1  1  1  1  1  1 -1 -1 -1 -1]\n",
            "Iteration 10 - Error: 0.191 - Predictions: [-1 -1 -1 -1  1  1  1  1  1  1]\n",
            "True labels      : [-1 -1 -1 -1  1  1 -1  1  1  1]\n",
            "AdaBoost Predictions: [-1. -1. -1. -1.  1.  1. -1.  1.  1.  1.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SAMME"
      ],
      "metadata": {
        "id": "aH8FojL_DpyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stands for *Stagewise Additive Modeling using a Multi-class Exponential loss function* ,is an extension of the AdaBoost algorithm to handle multi-class classification problems.\n",
        "\n",
        "The traditional AdaBoost algorithm was designed for binary classification. When it comes to multi-class classification problems, we need to adapt the method, and SAMME is one such adaptation.\n",
        "\n",
        "Here's a brief overview of SAMME:\n",
        "\n",
        "1. **Initialization**: As in AdaBoost, we initialize the weights for each data point.\n",
        "\n",
        "2. **For each iteration**:\n",
        "    - Train a weak learner on the weighted data.\n",
        "    - The learner predicts the class labels.\n",
        "    - Calculate the weighted error rate of the learner.\n",
        "    - Compute the learner's weight (alpha) based on the error. In SAMME, this weight is also influenced by the number of classes. For two classes, SAMME reduces to the original AdaBoost.\n",
        "    - Update the data point weights: this is done in a way similar to AdaBoost, but adapted for the multi-class scenario.\n",
        "\n",
        "3. **Final Model Output**: To make predictions, the output from the individual learners is combined based on their weights. The class with the highest combined weight is predicted.\n",
        "\n",
        "**SAMME.R**(R stands for \"Real\") is a variant of SAMME that uses the predicted class probabilities (from the weak learners) instead of just the class predictions, often leading to faster convergence.\n",
        "\n",
        "In practical terms, if you use libraries like scikit-learn, you'll see options to choose between AdaBoost and SAMME when you want to solve multi-class classification problems using boosting."
      ],
      "metadata": {
        "id": "1KTlUINyDn5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##sklearn AdaBoostClassifier"
      ],
      "metadata": {
        "id": "ixm12kYrEPRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"iris.csv\")"
      ],
      "metadata": {
        "id": "lSbL2ow3EsTx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "5xYBj919Eynp",
        "outputId": "134faf63-880f-4ec8-acf0-65a6275015f9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width species\n",
              "0           5.1          3.5           1.4          0.2  setosa\n",
              "1           4.9          3.0           1.4          0.2  setosa\n",
              "2           4.7          3.2           1.3          0.2  setosa\n",
              "3           4.6          3.1           1.5          0.2  setosa\n",
              "4           5.0          3.6           1.4          0.2  setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4597381-8529-494a-aae4-ed7959f7abf0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4597381-8529-494a-aae4-ed7959f7abf0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4597381-8529-494a-aae4-ed7959f7abf0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4597381-8529-494a-aae4-ed7959f7abf0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ae3cde91-18ee-471f-a94a-3099b01a372d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae3cde91-18ee-471f-a94a-3099b01a372d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ae3cde91-18ee-471f-a94a-3099b01a372d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= df.drop(\"species\", axis=1)\n",
        "y= df[\"species\"]"
      ],
      "metadata": {
        "id": "FKaxQ-dPEzaq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rrG0-1HpFMeu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "3SXVYbv8FcAn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "udyVrt1jFdPg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create and fit AdaBoostClassifier"
      ],
      "metadata": {
        "id": "i29PN763KJm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=200,\n",
        "    algorithm=\"SAMME.R\",\n",
        "    learning_rate=0.5)\n",
        "\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Ihz3ZbN7FuCL",
        "outputId": "d0b0f572-9adc-4ae8-8da3-aa03b252c4c5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predict"
      ],
      "metadata": {
        "id": "7awmriNZKSLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = ada_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "f4dLQCBKJPaU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "CGE39FcKJe-Y"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqf2fEnPJsl7",
        "outputId": "7bf37b56-8b57-48fa-8b46-354a78f4c96e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       0.88      1.00      0.94        15\n",
            "   virginica       1.00      0.88      0.93        16\n",
            "\n",
            "    accuracy                           0.96        50\n",
            "   macro avg       0.96      0.96      0.96        50\n",
            "weighted avg       0.96      0.96      0.96        50\n",
            "\n"
          ]
        }
      ]
    }
  ]
}