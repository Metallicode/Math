{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONQWYLV5sXrUDC2XmtUytl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Boosting_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Methods"
      ],
      "metadata": {
        "id": "VMbmrRWXmma6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Boosting works by combining multiple weak learners to create a strong learner. **A weak learner** is a model that performs only slightly better than random guessing, while **a strong learner** is a model that has high accuracy on the task.*\n",
        "\n",
        "* Sequential Learning: Unlike Random Forests, which train each tree independently, boosting methods train learners sequentially. Each subsequent model is built to correct the mistakes of its predecessor.\n",
        "\n",
        "* Weighted Training Data: After each round, instances that were misclassified by the current weak learner are given higher weights, making it more likely for subsequent learners to focus on them. Similarly, correctly classified instances are given lower weights.\n",
        "\n",
        "* Aggregation: The final prediction is typically a weighted combination of the predictions of all the weak learners. For regression tasks, this is often a weighted sum. For classification tasks, it can be a weighted majority vote.\n",
        "\n",
        "* Regularization via Shrinkage: Boosting algorithms often introduce a learning rate (or shrinkage) parameter. This slows down the learning process by shrinking the contribution of each weak learner, which often leads to better performance and less overfitting.\n",
        "\n",
        "* Strengths: Often provides higher accuracy than other algorithms, especially on tabular data.\n",
        "Effective with imbalanced datasets by focusing more on the underrepresented class.\n",
        "\n",
        "* Challenges: More prone to overfitting, especially with noisy data or when the dataset is small.\n",
        "Typically slower to train than bagging methods like Random Forests due to the sequential nature.\n",
        "Loses some interpretability when compared to a single decision tree.\n",
        "\n",
        "* Hyperparameters: Boosting methods come with their set of hyperparameters (like the depth of the trees, learning rate, and number of trees) that need to be tuned for optimal performance.\n",
        "\n",
        "*In essence, boosting is a versatile and powerful method in the realm of machine learning, particularly for structured/tabular data. It leverages the idea that \"the whole is greater than the sum of its parts,\" turning a series of weak models into a highly accurate combined model.*"
      ],
      "metadata": {
        "id": "DuVPTh7jeBZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Examples of Boosting Algorithms:\n",
        "\n",
        "> **AdaBoost (Adaptive Boosting)**: One of the first successful boosting algorithms. It adjusts the weights of misclassified instances and combines learners through weighted majority voting.\n",
        "\n",
        "> **Gradient Boosting Machines (GBM)**: Builds trees sequentially, where each tree tries to correct the residuals (the differences between the predicted and true values) of the previous one. It generalizes the boosting procedure to optimize arbitrary loss functions.\n",
        "\n",
        "> **XGBoost, LightGBM, CatBoost**: Modern and efficient implementations of gradient boosting that offer faster computation and additional features."
      ],
      "metadata": {
        "id": "srvJhuODe6vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Soft vs. Hard Voting"
      ],
      "metadata": {
        "id": "-oykD9n3hKoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When using ensemble methods, especially in the context of classification, there are two main ways to aggregate predictions from multiple classifiers: soft voting and hard voting.\n",
        "\n",
        "**Hard Voting:**\n",
        "> Each classifier in the ensemble \"votes\" for a class label.\n",
        "The class label that gets the majority of the votes is chosen as the final predicted class.\n",
        "Example: Suppose you have three classifiers and two possible classes (A and B). If the classifiers predict A, A, and B, respectively, then the ensemble prediction using hard voting would be A, as it's the majority class predicted.\n",
        "\n",
        "**Soft Voting:**\n",
        "\n",
        "> Each classifier provides a probability for each class label.\n",
        "The probabilities for each class are averaged across all classifiers.\n",
        "The class with the highest average probability is chosen as the final predicted class.\n",
        "Example: Using the same scenario with two possible classes (A and B), let's say the classifiers provide the following probabilities for class A: 0.7, 0.2, and 0.8. The average probability for class A would be (0.7 + 0.2 + 0.8) / 3 = 0.567. If the average for class B is lower than this, then the ensemble prediction using soft voting would be A.\n",
        "\n",
        "**Which to Use?:**\n",
        "\n",
        "**Soft voting** is generally preferred over hard voting, especially when the classifiers are well-calibrated. The reason is that soft voting takes into account the confidence levels of individual classifiers, leading to potentially more accurate ensemble predictions.\n",
        "\n",
        "**Hard voting** can be more robust in cases where the probability outputs from classifiers aren't reliable or well-calibrated.\n",
        "\n",
        "*In conclusion, while hard voting is a simple majority rule approach, soft voting provides a more nuanced way to aggregate predictions by considering the confidence of individual models."
      ],
      "metadata": {
        "id": "4JMEKojthIf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost"
      ],
      "metadata": {
        "id": "Kn5junWzkISR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "short for \"Adaptive Boosting\".\n",
        "\n",
        "one of the most popular ensemble methods that employs the boosting technique.\n",
        "\n",
        "\n",
        "**AdaBoost in Simple Terms:**\n",
        "\n",
        "> Imagine you're trying to differentiate between cats and dogs, but you're not very good at it. So, you ask a friend (a \"weak learner\") to help. Your friend makes some mistakes but gets some right.\n",
        "\n",
        "> Instead of getting upset at the mistakes, you focus more on the pictures your friend got wrong and ask another friend to try and classify those.\n",
        "\n",
        "> This second friend also makes mistakes, but again, instead of being upset, you focus on the pictures that are still wrong.\n",
        "\n",
        "> You repeat this process with many friends. Each friend tries to correct the mistakes of the previous one.\n",
        "\n",
        "> In the end, you combine all your friends' decisions. Each friend gets a say, but friends who were more confident and accurate have a louder voice in the final decision.\n",
        "\n",
        "*This process is basically AdaBoost! Each friend is a \"weak learner\" (often a simple decision tree). Mistakes made by previous learners are given more emphasis by subsequent learners. In the end, you combine all their decisions for a final, strong decision.*"
      ],
      "metadata": {
        "id": "laayFUe4kMV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost Formula"
      ],
      "metadata": {
        "id": "6PU_YF5rs6R2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Weight Initialization: Each data point is given an equal weight initially.\n",
        "\n",
        "2. For Each Learner:\n",
        "\n",
        "> * Fit the learner on the dataset considering the weights.\n",
        "* Calculate the error of the learner.\n",
        "* Compute the learner's weight in the final decision.\n",
        "Learners with lower error have more weight (a louder voice).\n",
        "* Increase the weights of the misclassified points, so the next learner pays more attention to them.\n",
        "\n",
        "3. Final Output: Combine the decisions of all learners. Each learner's decision is weighted by its accuracy."
      ],
      "metadata": {
        "id": "0gGJ_g6ks7W_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Error of a Learner"
      ],
      "metadata": {
        "id": "Rt3Ed9NRtjr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "learner_error = Sum_of_weights_of misclassified_points / Total_Sum_of_weights\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "s0_I7BABtm3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Weight of a learner (alpha)"
      ],
      "metadata": {
        "id": "J3s1ypFnuBe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "alpha = (1/2)*natural_log(1-error/error)\n",
        "```"
      ],
      "metadata": {
        "id": "c4YmeIr0uMo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Updating Weights of Data Points"
      ],
      "metadata": {
        "id": "Z8bDot4Ou6m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For correctly classified points:**\n",
        "\n",
        "```\n",
        "new_w = old_w * np.e**-alpha\n",
        "```\n",
        "\n",
        "**For misclassified points:**\n",
        "```\n",
        "new_w = old_w * np.e**alpha\n",
        "```"
      ],
      "metadata": {
        "id": "7Kj6U5F5u-3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final Prediction"
      ],
      "metadata": {
        "id": "nCW2Yinivvsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematically, if h_t(x) is the prediction of the t'th weak learner for an input x and α_t is the weight of that learner, then the aggregated score for x is given by\n",
        "\n",
        "```\n",
        "(Where T is the total number of weak learners)\n",
        "\n",
        "H(x) = sum(α_t*h_t(x) for t in T)\n",
        "\n",
        "Output = Sign(H(x))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "bW2he2AYwXls"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxfsLt1Jvkmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}