{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLn8TcksN266dwElehXdMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Intro_To_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intro To LSTM"
      ],
      "metadata": {
        "id": "QWGYh70T80lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Long Short-Term Memory (LSTM):**\n",
        "\n",
        "In simple terms, LSTM is a special type of Recurrent Neural Network (RNN) that is capable of learning and remembering over long sequences and is less susceptible to the vanishing gradient problem.\n",
        "\n",
        "**Why LSTMs?**\n",
        "\n",
        "The traditional RNNs suffer from the vanishing (and sometimes exploding) gradient problem which means, as the sequences get longer, RNNs tend to forget the earlier elements in the sequence. This makes them struggle in tasks requiring capturing long-term dependencies. LSTMs were introduced to combat these issues.\n",
        "\n",
        "**The Idea Behind LSTMs:**\n",
        "\n",
        "Think of LSTM as a cell that makes decisions about what information to store, and how to allow values to flow through it. These decisions are guided by gates. Specifically:\n",
        "\n",
        "1. **Forget Gate:** Decides what information from the cell state should be thrown away or kept.\n",
        "2. **Input Gate:** Updates the cell state with new information.\n",
        "3. **Output Gate:** Determines what value to output based on the cell state and the input.\n",
        "\n",
        "**LSTM Cell Structure:**\n",
        "\n",
        "An LSTM cell consists of the following main components:\n",
        "\n",
        "1. **Cell State:** Think of this as the \"memory\" of the cell. It can store long-term information.\n",
        "2. **Hidden State:** This is the output from the previous time step.\n",
        "3. **Gates:** These are a series of neural networks (usually sigmoid and tanh activations) that control the flow of information into and out of the memory cell. There are three primary gates:\n",
        "   - **Forget Gate (f):** Decides what information weâ€™re going to throw away from the cell state.\n",
        "   - **Input Gate (i and g):** The input gate decides which values will be updated in the state. It has two parts:\n",
        "     * A sigmoid layer which decides which values get updated.\n",
        "     * A tanh layer which creates a vector of new candidate values.\n",
        "   - **Output Gate (o):** Decides what value the cell will output based on the cell state.\n",
        "\n",
        "Mathematically, the gates are represented with their own weight matrices, and their values are computed using the current input and the previous hidden state.\n",
        "\n",
        "**In essence:** The beauty of LSTMs comes from their ability to remember from long-term sequences (which is probably why they're named as such), and they are well-suited for classifying, processing, and predicting time series given time lags of unknown size. They're widely used in tasks like machine translation, speech recognition, and more.\n",
        "\n",
        "A visualization of the LSTM cell can help a lot to understand its inner workings.\n",
        "\n",
        "\n",
        "[Chris Olah's blog](http://colah.github.io/)\n",
        "\n",
        "[Understanding-LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "[Andrej Karpathy blog](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
      ],
      "metadata": {
        "id": "mR4NbHpC9N--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X53g7I5Z8wpI"
      },
      "outputs": [],
      "source": []
    }
  ]
}