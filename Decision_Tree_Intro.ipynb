{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aSqDJNCKn9eS",
        "XcW-E14CsjDZ",
        "n2TBf92wuXpU",
        "2dhG5aLuzLjw"
      ],
      "authorship_tag": "ABX9TyP5FUJbpTxTLzPGrOcVfLkF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Decision_Tree_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entropy - the measure of randomness inside a system"
      ],
      "metadata": {
        "id": "aSqDJNCKn9eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy is a measure of unpredictability or randomness in a system. When talking about data, entropy is often used in the context of information theory to describe the amount of randomness or unpredictability in a set of values.\n",
        "\n",
        "For a discrete set of values, the entropy \\( H(X) \\) can be computed using the formula:\n",
        "\n",
        "**H(X) = - sum(  p(x_i) * log_2(p(x_i)) )**\n",
        "\n",
        "Here's what each part of the formula means:\n",
        "\n",
        "1. **\\( H(X) \\)**: This represents the entropy of the random variable \\( X \\). Entropy measures the average amount of information contained in each message produced by a stochastic source of data. In simpler terms, it quantifies the uncertainty (or randomness) involved in predicting the outcome of a random variable.\n",
        "\n",
        "2. **\\( p(x_i) \\)**: This is the probability of occurrence of an individual value \\( x_i \\). It represents how often \\( x_i \\) appears in the dataset.\n",
        "\n",
        "3. **\\( log_2(p(x_i)) \\)**: The logarithm (base 2) of the probability. The choice of the base 2 is because information is often measured in bits. The log function is used because it captures the concept that less probable events provide more information when they do occur.\n",
        "\n",
        "4. **The negative sign**: This is needed because the probabilities \\( p(x_i) \\) are always in the range [0, 1], and logarithm values in this range are negative. The negative sign ensures that entropy is positive.\n",
        "\n",
        "5. **sum()**: The sum is over all unique values in the set. We're averaging (or summing) the information from all possible outcomes.\n",
        "\n",
        "Here's a more intuitive explanation of the formula:\n",
        "\n",
        "- If an event has a high probability (it occurs very often), then its occurrence doesn't provide much new information (it's expected).\n",
        "\n",
        " The term **p(x_i) * log_2 p(x_i)** for that event will be close to zero.\n",
        "  \n",
        "- If an event has low probability (it rarely happens), its occurrence provides more information (it's surprising). This term will have a larger absolute value.\n",
        "\n",
        "- Entropy sums over all these terms, so it gives a measure of the average unpredictability or randomness in the dataset.\n",
        "\n",
        "Here's a simple Python function to compute entropy for a list of discrete values:\n",
        "\n",
        "The output will be the entropy of the data. The higher the entropy, the more unpredictable (or random) the data. If all values are the same, the entropy is zero, indicating no unpredictability."
      ],
      "metadata": {
        "id": "CNWg0OxPovTm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vQ4t8HshrEJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166734f2-ec06-4a3b-c220-84338edc8560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of the dataset: 0.9183\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def entropy(data):\n",
        "    # Compute frequency of each value\n",
        "    _, counts = np.unique(data, return_counts=True)\n",
        "\n",
        "    # Compute probabilities\n",
        "    probabilities = counts / counts.sum()\n",
        "\n",
        "    # Compute entropy\n",
        "    ent = -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "    return ent\n",
        "\n",
        "# Test variues datasets\n",
        "#data = [1, 1, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3]\n",
        "#data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "#data = [0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "#data = [0,0,0,0,0,0,1,0,0,0,0,0,0,0]\n",
        "#data = [1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2]\n",
        "#data = [1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3]\n",
        "#data = [1,1,2,1,1,2,1,1,2,1,1,2,1,1,2,1,1,2]\n",
        "\n",
        "print(f\"Entropy of the dataset: {entropy(data):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gini Impurity"
      ],
      "metadata": {
        "id": "XcW-E14CsjDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini impurity measures **the probability that a randomly chosen element would be incorrectly classified**. Like entropy, it's maximum when classes are balanced and is 0 when all elements are of the same class."
      ],
      "metadata": {
        "id": "ZWY56aNxsi9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "G(X)=1−∑[p(x_i)]**2\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "**X** is the dataset.\n",
        "\n",
        "**p(x_i)** is the proportion (or probability) of the samples that belong to class x_i in dataset X.\n",
        "The sum runs over all the unique classes in the dataset.\n",
        "\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "If all elements are from a single class, the Gini impurity is 0 (i.e., it's pure).\n",
        "If the elements are distributed uniformly across various classes, the Gini impurity reaches its maximum."
      ],
      "metadata": {
        "id": "qMYRtCq4tHMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(probs):\n",
        "    \"\"\"\n",
        "    Calculate the Gini impurity for a list of class probabilities.\n",
        "\n",
        "    :param probs: List of class probabilities.\n",
        "    :return: Gini impurity value.\n",
        "    \"\"\"\n",
        "    return 1 - sum([p**2 for p in probs])\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Binary class example where each class has equal probability\n",
        "probs1 = [0.5, 0.5]\n",
        "print(\"Gini impurity for probs1:\", gini_impurity(probs1))\n",
        "\n",
        "# 3-class example with probabilities of 0.2, 0.3, and 0.5\n",
        "probs2 = [0.2, 0.3, 0.5]\n",
        "print(\"Gini impurity for probs2:\", gini_impurity(probs2))\n",
        "\n",
        "# Another binary example where one class has 0.8 probability and the other 0.2\n",
        "probs3 = [0.8, 0.2]\n",
        "print(\"Gini impurity for probs3:\", gini_impurity(probs3))\n"
      ],
      "metadata": {
        "id": "YoPKohOxxHYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Usage in Decision Trees"
      ],
      "metadata": {
        "id": "n2TBf92wuXpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When growing a decision tree, at each step, the algorithm evaluates potential splits (using one feature and a threshold) to partition the data. For each potential split, it calculates the Gini impurity of the resulting child nodes. The split that produces the largest decrease in impurity (compared to the parent node) is chosen. This process is repeated recursively."
      ],
      "metadata": {
        "id": "NJ-U6TFquV5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The maximum value of Gini impurity depends on the number of classes in your dataset. For a binary classification problem (two classes), the maximum Gini impurity is 0.5. This occurs when instances are evenly split between the two classes. In other words, \\( p(x_1) = 0.5 \\) and \\( p(x_2) = 0.5 \\), leading to:\n",
        "\n",
        "**G(X) = 1 - [0.5^2 + 0.5^2] = 0.5**\n",
        "\n",
        "For a general scenario with c classes that have an equal probability:\n",
        "\n",
        "\n",
        "**G(X) = 1 - c * ( 1/c )^2 = 1 - 1/c**\n",
        "\n",
        "\n",
        "For example:\n",
        "- 2 classes: G(X) = 0.5\n",
        "- 3 classes: G(X) approx 0.67\n",
        "- 4 classes: G(X) = 0.75\n",
        "- And so on...\n",
        "\n",
        "The maximum Gini impurity increases as the number of classes increases, but it will always be less than 1. The value approaches 1 as the number of classes becomes very large, and each class has an equal probability."
      ],
      "metadata": {
        "id": "Q0hdoZGEva8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A Dcision Stump (1-level decision tree)"
      ],
      "metadata": {
        "id": "2dhG5aLuzLjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a rudimentary decision tree classifier that splits on a single feature to minimize the Gini impurity. We'll use a binary classification task for simplicity.\n",
        "\n",
        "Here's a breakdown of the process:\n",
        "\n",
        "We'll have a dataset with labels 0 or 1.\n",
        "We'll attempt to find the best split on a single feature to segregate the two classes.\n",
        "The best split will be determined by the lowest Gini impurity.\n",
        "\n",
        "\n",
        "The code defines a decision stump (1-level decision tree). It splits based on a single feature's value. The code finds the best value to split on (the \"threshold\") to segregate the given labels into two groups with the lowest combined Gini impurity.\n",
        "\n",
        "\n",
        "Remember that in a real-world scenario, decision trees consider multiple features, split on more than one level, and require more comprehensive data management techniques (like handling missing values, categorical features, etc.). The example provided is an over-simplification to demonstrate the concept.\n"
      ],
      "metadata": {
        "id": "5Aqh33iSy6aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gini_impurity(labels):\n",
        "    \"\"\"\n",
        "    Calculate the Gini impurity for a list of labels.\n",
        "    \"\"\"\n",
        "    # Count the occurrences of each label\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    probs = counts / len(labels)\n",
        "\n",
        "    return 1 - sum(p**2 for p in probs)\n",
        "\n",
        "\n",
        "def best_split(feature, labels):\n",
        "    \"\"\"\n",
        "    Find the best split value for a feature using Gini impurity.\n",
        "    \"\"\"\n",
        "    min_impurity = float('inf')\n",
        "    best_threshold = None\n",
        "\n",
        "    # Sort the unique values to consider as split candidates\n",
        "    for value in np.unique(feature):\n",
        "        left_mask = feature < value\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        # Calculate weighted Gini impurity\n",
        "        left_impurity = gini_impurity(labels[left_mask])\n",
        "        right_impurity = gini_impurity(labels[right_mask])\n",
        "\n",
        "        weighted_impurity = (np.sum(left_mask) * left_impurity + np.sum(right_mask) * right_impurity) / len(labels)\n",
        "\n",
        "        if weighted_impurity < min_impurity:\n",
        "            min_impurity = weighted_impurity\n",
        "            best_threshold = value\n",
        "\n",
        "    return best_threshold, min_impurity\n",
        "\n",
        "\n",
        "# Example dataset\n",
        "# Features: [feature_1]\n",
        "features = np.array([2, 3, 10, 19, 13, 18, 40, 16, 24, 23])\n",
        "labels = np.array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1])\n",
        "\n",
        "threshold, impurity = best_split(features, labels)\n",
        "print(f\"Best split threshold: {threshold}, Gini impurity: {impurity:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-eg0yNkzdJw",
        "outputId": "7ce64ec1-9310-452d-d704-6f49499d800d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best split threshold: 18, Gini impurity: 0.160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90VpOKFUzgUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}