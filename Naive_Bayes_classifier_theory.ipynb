{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0AQGN9bCaBvN",
        "72rvjeM0eVJG",
        "B-uuHWbtiFO1",
        "_7WY7Ke6hqgQ"
      ],
      "authorship_tag": "ABX9TyN8YqQZgRQ5VmMVDV1a8wFM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metallicode/Math/blob/main/Naive_Bayes_classifier_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes classifier"
      ],
      "metadata": {
        "id": "u92PZI1LXZc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes classifier is based on Bayes' Theorem, a fundamental concept in probability theory and statistics. Let's break this down step by step.\n",
        "\n",
        "**1. Bayes' Theorem**\n",
        "\n",
        "Bayes' Theorem describes the probability of an event, based on prior knowledge of conditions related to the event. Mathematically, it's defined as:\n",
        "\n",
        "**P(A|B) = P(B|A)*P(A) / P(B)**\n",
        "\n",
        "Where:\n",
        "- P(A|B) is the posterior probability of A occurring given B.\n",
        "- P(B|A) is the likelihood which is the probability of B occurring given A.\n",
        "- P(A) and  P(B) are the prior probabilities of A and B respectively.\n",
        "\n",
        "**2. Application to Classification:**\n",
        "\n",
        "Imagine you want to classify a piece of text as either positive or negative sentiment. Here, A is the class (positive/negative) and B is the observed text.\n",
        "\n",
        "Using Bayes' Theorem, the probability that a text belongs to the Positive class is:\n",
        "\n",
        "P(Positive|Text) = P(Text|Positive) * P(Positive) / P(Text)\n",
        "\n",
        "And similarly for the Negative class.\n",
        "\n",
        "**3. The \"Naive\" Assumption:**\n",
        "\n",
        "The word \"naive\" in Naive Bayes comes from the assumption that each feature in the dataset is independent of all other features. This is a strong and usually unrealistic assumption, hence the name.\n",
        "\n",
        "For the text example, it means that the presence of each word in the text is independent of the presence of any other word, which isn't always true. But despite this simplifying assumption, Naive Bayes often performs surprisingly well.\n",
        "\n",
        "**4. For Classification:**\n",
        "\n",
        "You would compute the probability of the text being in each class, then assign the class with the highest probability.\n",
        "\n",
        "For example, if P(Positive|Text) > P(Negative|Text), then classify the text as Positive.\n",
        "\n",
        "**5. Simplifying Further:**\n",
        "\n",
        "The denominator P(Text) is constant for all classes, so for the purpose of classification, we can ignore it. We're only interested in which numerator is largest.\n",
        "\n",
        "Therefore, the decision rule simplifies to comparing:\n",
        "\n",
        "**P(Text|Positive) * P(Positive)**\n",
        "\n",
        "versus:\n",
        "\n",
        "**P(Text|Negative) * P(Negative)**\n",
        "\n",
        "**6. Example with Numbers:**\n",
        "\n",
        "Let's imagine a toy scenario:\n",
        "- 60% of our texts are Positive, so P(Positive) = 0.6\n",
        "- 40% are Negative, so P(Negative) = 0.4\n",
        "- The word \"awesome\" appears in 80% of Positive texts and 10% of Negative texts.\n",
        "\n",
        "Given a new text \"This movie is awesome\", what class should it belong to?\n",
        "\n",
        "P(Positive|awesome) ∝ 0.8 * 0.6 = 0.48\n",
        "\n",
        "\n",
        "P(Negative|awesome) ∝ 0.1 * 0.4 = 0.04\n",
        "\n",
        "Since 0.48 > 0.04, we classify the text as Positive.\n",
        "\n",
        "**Conclusion:**\n",
        "This is a highly simplified introduction to the math behind Naive Bayes. In practice, additional techniques such as \"smoothing\" are employed to handle issues like zero probabilities for unseen words."
      ],
      "metadata": {
        "id": "oBtDNayUXR0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ∝ \"is proportional to\""
      ],
      "metadata": {
        "id": "0AQGN9bCaBvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The symbol ∝ is read as \"is proportional to.\" It indicates that two quantities have a constant ratio or a constant multiplicative relationship.\n",
        "\n",
        "In the context provided (Naive Bayes)\n",
        "\n",
        "(Positive|awesome) ∝ 0.8 * 0.6\n",
        "\n",
        "this means that the probability\n",
        "P(Positive|awesome) is proportional to 0.8 * 0.6, but not necessarily equal to it.\n",
        "\n",
        "The reason for this is because in Naive Bayes classification, we're typically only concerned with the relative magnitudes of probabilities, not their absolute values. Thus, the normalization factor (the denominator in Bayes' theorem) is often left out for simplicity, and we use the proportional symbol to indicate this."
      ],
      "metadata": {
        "id": "CBqtnKAjZZJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Joint Probabilities"
      ],
      "metadata": {
        "id": "72rvjeM0eVJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's delve deeper into the concept of joint probabilities and the complications that arise without the independence assumption in the context of the Naive Bayes classifier.\n",
        "\n",
        "1. **Joint Probability**:\n",
        "   The joint probability of multiple events is the probability of all those events occurring simultaneously. For instance, consider two features \\( A \\) and \\( B \\). The joint probability \\( P(A, B) \\) represents the probability that \\( A \\) and \\( B \\) occur together.\n",
        "\n",
        "2. **Number of Joint Probabilities Without Independence**:\n",
        "   If we don't make the independence assumption, we need to consider the joint probabilities of all feature combinations. With \\( n \\) binary features, there are \\( 2^n \\) possible combinations, meaning you'd have to estimate \\( 2^n \\) joint probabilities. As \\( n \\) grows, this number explodes. For continuous or multi-class features, the situation becomes even more complex.\n",
        "\n",
        "3. **Example with Binary Features**:\n",
        "   Let's assume you're building a Naive Bayes classifier to predict if an email is spam or not, and you have 3 binary features:\n",
        "   - \\( F_1 \\): Whether the email contains the word \"sale\" (1 if it does, 0 otherwise).\n",
        "   - \\( F_2 \\): Whether the email contains the word \"free\" (1 if it does, 0 otherwise).\n",
        "   - \\( F_3 \\): Whether the email has an attachment (1 if it does, 0 otherwise).\n",
        "   \n",
        "   Without the independence assumption, you'd need to estimate the joint probabilities for all combinations of these features, such as:\n",
        "   - \\( P(F_1 = 1, F_2 = 1, F_3 = 1) \\)\n",
        "   - \\( P(F_1 = 0, F_2 = 1, F_3 = 1) \\)\n",
        "   - \\( P(F_1 = 1, F_2 = 0, F_3 = 1) \\)\n",
        "   - ...\n",
        "   - \\( P(F_1 = 0, F_2 = 0, F_3 = 0) \\)\n",
        "   \n",
        "   That's \\( 2^3 = 8 \\) joint probabilities just for 3 features! Imagine if you had 100 features; you'd need to estimate \\( 2^{100} \\) joint probabilities!\n",
        "\n",
        "4. **Simplified with Independence**:\n",
        "   With the independence assumption, the joint probability can be decomposed into individual probabilities:\n",
        "   \\( P(A, B) = P(A) * P(B) \\)\n",
        "   Extending this to our email example, instead of computing the 8 joint probabilities, we'd compute individual probabilities like \\( P(F_1 = 1) \\), \\( P(F_2 = 1) \\), and \\( P(F_3 = 1) \\). This drastically reduces computational complexity.\n",
        "\n",
        "In essence, the independence assumption in Naive Bayes is a trade-off. We introduce a potential source of inaccuracy (because features in real-world data are rarely truly independent) in exchange for a significant simplification in our calculations and a model that remains tractable even with a large number of features."
      ],
      "metadata": {
        "id": "0FGYBOcqeKUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Joint probability of events"
      ],
      "metadata": {
        "id": "B-uuHWbtiFO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notation P(A, B, C) denotes the joint probability of events A, B, and C occurring simultaneously. In other words, it represents the probability that all three of these events happen at the same time.\n",
        "\n",
        "Let's break this down:\n",
        "\n",
        "- P(A): Probability of event A occurring.\n",
        "- P(B): Probability of event B occurring.\n",
        "- P(C): Probability of event C occurring.\n",
        "- P(A, B): Joint probability of events A and B occurring together.\n",
        "- P(A, B, C): Joint probability of events A, B, and C all occurring together.\n",
        "\n",
        "For example, consider you have a deck of cards, and you're drawing three cards consecutively (without replacement):\n",
        "\n",
        "- Let A be the event that the first card is an Ace.\n",
        "- Let B be the event that the second card is a King.\n",
        "- Let C be the event that the third card is a Queen.\n",
        "\n",
        "Then, P(A, B, C) would represent the probability that the first card you draw is an Ace, the second is a King, and the third is a Queen.\n",
        "\n",
        "The joint probability can often be represented as a product of conditional probabilities using the chain rule, as I explained in the previous answer:\n",
        "\n",
        "**P(A, B, C) = P(A|B, C) * P(B|C) * P(C)**\n",
        "\n",
        "However, when the events are independent:\n",
        "\n",
        "**P(A, B, C) = P(A) * P(B) * P(C)**\n",
        "\n",
        "This understanding of joint probabilities is foundational in many areas of probability and statistics, including the operation of many machine learning algorithms."
      ],
      "metadata": {
        "id": "OBm3D3ktiCQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chain rule"
      ],
      "metadata": {
        "id": "_7WY7Ke6hqgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The Bayes' theorem is a fundamental concept in probability theory and statistics that describes the probability of an event, based on prior knowledge of related events. Mathematically:\n",
        "\n",
        "**P(A|B) = P(B|A)* P(A) / P(B)**\n",
        "\n",
        "Where:\n",
        "- P(A|B) is the posterior probability of A given B.\n",
        "- P(B|A) is the likelihood, which is the probability of B given A.\n",
        "- P(A) is the prior probability of A.\n",
        "- P(B) is the total probability of B.\n",
        "\n",
        "**Applying Bayes' Theorem to Classification:**\n",
        "\n",
        "In the context of classification, suppose we have a data point with features F and we want to determine the probability it belongs to class C:\n",
        "\n",
        "P(C|F) ∝ P(F|C) * P(C)\n",
        "\n",
        "Here, we're interested in finding the class C that maximizes P(C|F). This is found by looking at the product of:\n",
        "- P(F|C): How likely is the feature set F given the class C.\n",
        "- P(C): The prior probability of class C.\n",
        "\n",
        "**Chain Rule of Probability:**\n",
        "\n",
        "The chain rule of probability (or the product rule) is a fundamental rule providing a way to get the joint probability of a collection of events from conditional probabilities. It states:\n",
        "\n",
        "P(A, B) = P(A|B) * P(B)\n",
        "P(A, B, C) = P(A|B, C) * P(B|C) * P(C)\n",
        "... and so forth.\n",
        "\n",
        "\n",
        "**Applying the Chain Rule to our Classifier:**\n",
        "\n",
        "Let's assume for simplicity that our data point has two features F_1 and F_2. Using the chain rule:\n",
        "\n",
        "\n",
        "**P(F_1, F_2|C) = P(F_1|F_2, C) * P(F_2|C)**\n",
        "\n",
        "\n",
        "However, here's where the \"Naive\" in Naive Bayes comes in. We make an assumption that each feature is independent of every other feature given the class. So, for our example:\n",
        "\n",
        "\n",
        "**P(F_1|F_2, C) = P(F_1|C)**\n",
        "\n",
        "\n",
        "Hence, the joint probability can be simplified to:\n",
        "\n",
        "\n",
        "**P(F_1, F_2|C) = P(F_1|C) * P(F_2|C)**\n",
        "\n",
        "If you have more features, you continue multiplying these probabilities for each feature. The reason we use this is that estimating each P(F_i|C) individually (with the naive assumption) is much easier and requires less data than estimating the joint probabilities directly without this assumption.\n",
        "\n",
        "In conclusion, the Naive Bayes classifier combines the Bayes' theorem with the chain rule and a simplifying naive assumption of independence among features to create an efficient and often surprisingly accurate classification method."
      ],
      "metadata": {
        "id": "FAIkhiB6hfjz"
      }
    }
  ]
}